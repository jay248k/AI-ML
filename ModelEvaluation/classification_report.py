from sklearn.metrics import classification_report

y_true = [0, 1, 1, 0, 1]
y_pred = [0, 1, 0, 0, 1]

print(classification_report(y_true, y_pred))

#               precision    recall  f1-score   support

#            0       0.67      1.00      0.80         2
#            1       1.00      0.67      0.80         3

#     accuracy                           0.80         5
#    macro avg       0.83      0.83      0.80         5
# weighted avg       0.87      0.80      0.80         5

# 1ï¸âƒ£ What is classification_report?

# It gives a complete summary of classification metrics in one table:

# âœ” Precision
# âœ” Recall
# âœ” F1-score
# âœ” Support

# Instead of calculating each metric separately

# 3ï¸âƒ£ What Each Column Means
# ğŸ”¹ Precision

# Correct positive predictions.

# ğŸ”¹ Recall

# How many actual positives were detected.

# ğŸ”¹ F1-score

# Balance between precision & recall.

# ğŸ”¹ Support

# Number of actual samples in that class.

# ğŸŸ£ 4ï¸âƒ£ Why It Is VERY Useful

# Instead of writing:

# accuracy_score

# precision_score

# recall_score

# f1_score

# You can get everything in one function.

# Very common in projects and interviews ğŸ”¥

# ğŸ”´ 5ï¸âƒ£ Important for Imbalanced Data

# In Fraud Detection:

# Class 0 â†’ Genuine (large)

# Class 1 â†’ Fraud (small)

# Accuracy may look high
# But classification_report shows:

# Low recall for fraud

# Low precision for fraud

# Companies like Visa and PayPal analyze per-class performance carefully.

# ğŸŸ¤ 6ï¸âƒ£ Macro vs Weighted Average (Interview Question)
# ğŸ”¹ Macro Avg

# Average of metrics for all classes equally.

# Good when classes are balanced.

# ğŸ”¹ Weighted Avg

# Average weighted by support (number of samples).

# Better for imbalanced datasets.

# Very important interview question ğŸ”¥

# ğŸŸ  7ï¸âƒ£ Multiclass Example

# If 3 classes:

# classification_report will show metrics for:

# Class 0

# Class 1

# Class 2

# And overall averages.

# ğŸ§  8ï¸âƒ£ Interview Questions

# Q1: What does support mean?
# â†’ Number of true samples for each class.

# Q2: Why is classification_report better than accuracy?
# â†’ It shows per-class performance.

# Q3: What is macro average?
# â†’ Simple average of all classes.

# Q4: What is weighted average?
# â†’ Average considering class imbalance.

# ğŸ”¥ Final Understanding

# classification_report = Complete evaluation summary of classification model.

# It is built using:

# âœ” Confusion Matrix
# âœ” Precision
# âœ” Recall
# âœ” F1-score