from sklearn.metrics import accuracy_score

y_true = [0, 1, 1, 0, 1]
y_pred = [0, 1, 0, 0, 1]

acc = accuracy_score(y_true, y_pred)
print("Accuracy:", acc)

# accuracy_score in Machine Learning

# accuracy_score is a function from:

# sklearn.metrics

# It calculates:

# Accuracy=Correct Predictions / Total Predictions

# 1ï¸âƒ£ Simple Meaning

# If model predicted 100 samples
# and 90 are correct

# Accuracy = 90 / 100 = 0.90 (90%)

# ğŸŸ¡ 2ï¸âƒ£ Formula

# Accuracy = (TP + TN) / (TP + TN + FP + FN)

# Where:

# TP â†’ True Positive

# TN â†’ True Negative

# FP â†’ False Positive

# FN â†’ False Negative

# 4ï¸âƒ£ When Accuracy is Good?

# âœ” When dataset is balanced
# âœ” When both classes are equally important

# Example:

# Cat vs Dog classification

# Digit recognition

# ğŸ”´ 5ï¸âƒ£ When Accuracy is BAD?

# Very important for interview âš 

# In Fraud Detection:

# Suppose:

# 1000 transactions
# 990 genuine
# 10 fraud

# If model predicts:

# All transactions = genuine

# Accuracy = 990 / 1000 = 99%

# But it detected 0 fraud âŒ

# This is why companies like Visa and PayPal do NOT rely only on accuracy.

# They focus more on:

# âœ” Recall
# âœ” Precision
# âœ” F1-score
# âœ” ROC-AUC

# ğŸŸ¤ 6ï¸âƒ£ Binary vs Multiclass

# accuracy_score works for:

# âœ” Binary classification
# âœ” Multiclass classification

# Example (Multiclass):

# y_true = [0, 1, 2, 1]
# y_pred = [0, 2, 2, 1]

# It still calculates correctly predicted labels.

# ğŸŸ  7ï¸âƒ£ Interview Questions

# Q1: What is accuracy?
# â†’ Ratio of correct predictions to total predictions.

# Q2: Why is accuracy not good for imbalanced data?
# â†’ Because model can ignore minority class and still get high accuracy.

# Q3: What is better than accuracy in fraud detection?
# â†’ Recall, F1-score, ROC-AUC.

# ğŸ”¥ Final Summary

# accuracy_score is:

# âœ” Simple
# âœ” Easy to understand
# âœ” Good for balanced datasets
# âŒ Misleading for imbalanced datasets