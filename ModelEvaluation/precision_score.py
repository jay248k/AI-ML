from sklearn.metrics import precision_score

y_true = [0, 1, 1, 0, 1]
y_pred = [0, 1, 0, 0, 1]

precision = precision_score(y_true, y_pred)
print("Precision:", precision)

# 1ï¸âƒ£ Simple Meaning

# Precision answers:

# ğŸ‘‰ Out of all predicted Positive cases, how many were actually correct?

# Formula:
# Precision = TP / (TP + FP)

# Where:

# TP â†’ True Positive

# FP â†’ False Positive

# ğŸŸ¡ 2ï¸âƒ£ Easy Example

# Imagine:

# Model predicted 20 transactions as Fraud.

# But actually:

# 15 were real fraud âœ…

# 5 were normal transactions âŒ

# Precision = 15 / (15 + 5) = 0.75 (75%)

# Meaning:

# When model says â€œFraudâ€,
# 75% of the time it is correct.

# 4ï¸âƒ£ When Precision is Important?

# Precision is important when:

# â— False Positives are costly.

# Example:

# ğŸ’³ Fraud Detection

# If system wrongly blocks genuine customer:

# Customer gets angry ğŸ˜¡
# Bad experience

# Companies like PayPal and Mastercard care about this.

# High precision = fewer false alarms.

# ğŸ”´ 5ï¸âƒ£ Precision vs Recall (Important Difference)

# +-----------+-----------------------+
# | Metric    | Focus                 |
# +-----------+-----------------------+
# | Precision | Avoid False Positives |
# | Recall    | Avoid False Negatives |
# +-----------+-----------------------+

# Example:

# Medical Test:

# Precision â†’ When test says cancer, how often correct?

# Recall â†’ How many actual cancer cases detected?

# ğŸŸ¤ 6ï¸âƒ£ Imbalanced Dataset Case

# In fraud detection:

# If precision is high â†’ Model is careful before labeling fraud.

# If recall is high â†’ Model catches most fraud.

# Usually we balance both using:

# ğŸ‘‰ F1 Score

# 7ï¸âƒ£ Multiclass Precision

precision_score(y_true, y_pred, average='macro')

# Types of averaging:

# 'micro'

# 'macro'

# 'weighted'

# Very common interview question ğŸ”¥

# ğŸ§  8ï¸âƒ£ Interview Questions

# Q1: What does precision measure?
# â†’ Correct positive predictions.

# Q2: Formula?
# â†’ TP / (TP + FP)

# Q3: When is precision more important than recall?
# â†’ When False Positives are costly.

# Q4: Precision in fraud detection means?
# â†’ When model says fraud, how often correct.

# ğŸ”¥ Final Understanding

# Precision = Model honesty when predicting Positive.

# High precision = Fewer false alarms.
# Low precision = Many wrong positive predictions.