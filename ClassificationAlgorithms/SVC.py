import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Pass":  [0,0,0,0,1,1,1,1]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Pass"]

# Scaling is IMPORTANT for SVM
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

model = SVC(kernel="linear")
model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# SVC (Support Vector Classifier)

# SVC is the classification version of Support Vector Machine (SVM).

# This is very important for interviews.

# ğŸŸ¢ 1ï¸âƒ£ VERY EASY EXPLANATION

# Imagine two types of students:

# Pass ğŸ”µ
# Fail ğŸ”´

# We want to draw a line that separates them.

# Many lines can separate themâ€¦

# But SVM chooses:

# ğŸ‘‰ The BEST line
# ğŸ‘‰ With maximum margin

# Margin = Distance between line and nearest points.

# So:

# SVM = Find the widest possible separation line.

# ğŸŸ¡ 2ï¸âƒ£ WHAT IS MARGIN?

# Imagine:

# ğŸ”´ ğŸ”´ ğŸ”´ | ğŸ”µ ğŸ”µ ğŸ”µ

# That vertical line separates classes.

# Margin = Distance between line and closest red & blue points.

# SVM tries to:

# Maximize that margin.

# Because:

# Bigger margin â†’ Better generalization â†’ Less overfitting.

# ğŸ”µ 3ï¸âƒ£ WHAT ARE SUPPORT VECTORS?

# The closest points to boundary are called:

# ğŸ‘‰ Support Vectors

# These points decide the boundary.

# If you remove other points, boundary does not change much.

# Very important concept ğŸ”¥

# ğŸŸ£ 4ï¸âƒ£ MATHEMATICAL IDEA (Simplified)

# Decision boundary:

# w.x + b = 0

# SVM tries to:

# Minimize:

# 1/2||w||â‚‚

# Subject to correct classification constraints.

# Donâ€™t worry too much about math now.

# Concept is more important.

# ğŸŸ¤ 5ï¸âƒ£ WHAT IF DATA IS NOT LINEAR?

# Example:

# Points arranged in circle shape.

# We cannot separate with straight line.

# Solution:

# ğŸ‘‰ Kernel Trick

# Kernel transforms data into higher dimension.

# Common kernels:

# linear
# poly (Polynomial)
# rbf (Radial Basis Function) â† Most popular
# sigmoid

# 7ï¸âƒ£ IMPORTANT PARAMETERS

# kernel â†’ linear, rbf, poly

# C â†’ Regularization parameter

# Small C â†’ Large margin, more tolerant to mistakes
# Large C â†’ Smaller margin, less tolerant

# gamma (for rbf kernel)

# Controls influence of single training point.

# âš« 8ï¸âƒ£ VERY IMPORTANT: SCALING REQUIRED

# YES âœ”

# SVM is distance-based.

# Always scale data before SVM.

# ğŸŸ  9ï¸âƒ£ LOGISTIC REGRESSION vs SVM

# +----------------------+-----------------+
# | Logistic Regression  | SVM             |
# +----------------------+-----------------+
# | Probability-based    | Margin-based    |
# | Uses sigmoid         | Uses max margin |
# | Faster               | Slower          |
# | Linear boundary      | Can use kernel  |
# +----------------------+-----------------+

# 10ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What is Support Vector?
# Closest data points to decision boundary.

# Q2: What does C parameter do?
# Controls tradeoff between margin size and classification error.

# Q3: What is kernel trick?
# Mapping data into higher dimension.

# Q4: Does SVM need scaling?
# Yes, very important.

# Q5: When to use SVM?
# Medium-sized datasets, clear separation.

# ğŸŸ¢ 11ï¸âƒ£ WHEN TO USE SVM?

# Use when:

# âœ” Data is high dimensional
# âœ” Clear margin of separation
# âœ” Dataset is medium size
# âœ” Need strong classifier

# Avoid when:

# âŒ Very large dataset (slow)
# âŒ Many noise points

# ğŸŸ£ 12ï¸âƒ£ REAL PROJECT (Your Job Portal)

# Predict:

# Selected / Not Selected

# Features:

# Experience
# Skill Score
# Projects
# CGPA

# If classes are well separated â†’ SVM works very well.

# ğŸ”¥ 13ï¸âƒ£ SUMMARY

# SVM:

# âœ” Finds best separating boundary
# âœ” Maximizes margin
# âœ” Uses support vectors
# âœ” Uses kernels for non-linear data
# âœ” Needs scaling