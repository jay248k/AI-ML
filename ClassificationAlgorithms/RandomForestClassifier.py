import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Pass":  [0,0,0,0,1,1,1,1]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Pass"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=3,
    random_state=42
)

model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# ğŸŒ²ğŸŒ² RandomForestClassifier

# It is an advanced version of DecisionTreeClassifier.

# Very commonly used in real-world ML.

# ğŸŸ¢ 1ï¸âƒ£ VERY EASY EXPLANATION

# Decision Tree = One interviewer deciding

# Random Forest = 100 interviewers voting together ğŸ˜

# Each tree gives prediction:

# Selected / Not Selected

# Final answer = Majority vote

# So:

# RandomForestClassifier = Many decision trees + Voting

# ğŸŸ¡ 2ï¸âƒ£ WHY WE NEED IT?

# Problem with single Decision Tree:

# âŒ Overfits
# âŒ High variance
# âŒ Unstable

# Random Forest fixes this by:

# âœ” Using many trees
# âœ” Using random data samples
# âœ” Using random features

# Result:

# More accurate
# More stable
# Less overfitting

# ğŸ”µ 3ï¸âƒ£ HOW IT WORKS

# Step 1: Take random sample from dataset (Bootstrap)

# Step 2: Build a decision tree

# Step 3: At each split, choose random subset of features

# Step 4: Repeat many times (100+ trees)

# Step 5: Final prediction = Majority vote

# For example:

# Tree 1 â†’ Selected
# Tree 2 â†’ Selected
# Tree 3 â†’ Not Selected

# Final â†’ Selected (2 votes)

# 5ï¸âƒ£ IMPORTANT PARAMETERS

# n_estimators â†’ Number of trees

# max_depth â†’ Tree depth

# min_samples_split â†’ Minimum samples to split

# min_samples_leaf â†’ Minimum samples per leaf

# max_features â†’ Features used at each split

# More trees â†’ Better accuracy (usually) but slower

# âš« 6ï¸âƒ£ DOES IT NEED SCALING?

# No âŒ

# Trees do not use distance or gradients.

# Very important interview point ğŸ”¥

# ğŸŸ  7ï¸âƒ£ ADVANTAGES

# âœ” Very high accuracy
# âœ” Handles non-linear data
# âœ” Reduces overfitting
# âœ” Works with many features
# âœ” Gives feature importance

# ğŸŸ¤ 8ï¸âƒ£ DISADVANTAGES

# âŒ Slower than single tree
# âŒ Large memory usage
# âŒ Harder to interpret

# ğŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What is bagging?
# Training multiple models on random subsets and averaging/voting.

# Q2: Why does Random Forest reduce overfitting?
# Because averaging multiple trees reduces variance.

# Q3: What is OOB score?
# Out-of-Bag score (internal validation using unused samples).

# Q4: Does Random Forest perform feature selection?
# Yes, indirectly using feature importance.

# Q5: Difference between Random Forest and Decision Tree?

# Decision Tree â†’ One tree
# Random Forest â†’ Many trees

# ğŸŸ¢ 10ï¸âƒ£ FEATURE IMPORTANCE

# You can check:

# model.feature_importances_

# In your Job Portal example:

# Experience â†’ 0.45
# Skill Score â†’ 0.30
# Projects â†’ 0.15
# CGPA â†’ 0.10

# This helps HR understand key factors.

# 11ï¸âƒ£ DECISION TREE vs RANDOM FOREST

# +----------------+-------------------+
# | Decision Tree  | Random Forest     |
# +----------------+-------------------+
# | One tree       | Many trees        |
# | Overfits easily| Less overfitting  |
# | Fast           | Slower            |
# | Simple         | More accurate     |
# +----------------+-------------------+

# 12ï¸âƒ£ REAL PROJECT CONNECTION

# In your MERN + ML Job Portal:

# Use RandomForestClassifier to predict:

# Will candidate be selected?

# Features:

# Experience
# Skill Score
# Projects
# Certifications
# CGPA

# It will give:

# 0 â†’ Not selected
# 1 â†’ Selected

# Plus probability.

# Very practical model ğŸ”¥

# ğŸš€ 13ï¸âƒ£ SUMMARY

# RandomForestClassifier:

# âœ” Ensemble method
# âœ” Uses bagging
# âœ” Majority voting
# âœ” High accuracy
# âœ” No scaling needed