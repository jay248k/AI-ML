import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Pass":  [0,0,0,0,1,1,1,1]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Pass"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LogisticRegression()
model.fit(X_train, y_train)

predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)

print("Predictions:", predictions)
print("Probabilities:", probabilities)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Linear Regression â†’ Predicts numbers (like salary)

# Logistic Regression â†’ Predicts categories (like Pass/Fail)

# Example:

# If student studies 5 hours
# Will he PASS or FAIL?

# Output:

# 0 = Fail
# 1 = Pass

# So Logistic Regression predicts probability.

# ðŸŸ¡ 2ï¸âƒ£ WHY CANâ€™T WE USE LINEAR REGRESSION?

# Linear Regression gives output like:

# -2
# 1.7
# 10

# But probability must be between:

# 0 and 1

# So we use a special function:

# ðŸ‘‰ Sigmoid Function

# ðŸ”µ 3ï¸âƒ£ SIGMOID FUNCTION (Heart of Logistic Regression)

# Formula:

# Ïƒ(z)=1/(1+e^(-z))

# Where:

# z = (w1*x1 + w2*x2 + ... + wn*xn) + b

# Sigmoid converts any number into:

# 0 to 1 range

# Graph shape: S-shaped curve

# If output > 0.5 â†’ Class 1
# If output < 0.5 â†’ Class 0

# ðŸŸ£ 4ï¸âƒ£ HOW MODEL LEARNS?

# Linear regression uses MSE.

# Logistic regression uses:

# ðŸ‘‰ Log Loss (Binary Cross Entropy)

# Formula:

# Log Loss = -(y*log(p) + (1-y)*log(1-p))

# Why?

# Because we are predicting probabilities.

# 6ï¸âƒ£ LINEAR vs LOGISTIC REGRESSION
# +----------------------+-------------------+
# | Linear               | Logistic          |
# +----------------------+-------------------+
# | Predict numbers      | Predict classes   |
# | Uses MSE             | Uses Log Loss     |
# | Straight line        | S-shaped curve    |
# | No probability output| Gives probability |
# +----------------------+-------------------+

# 7ï¸âƒ£ TYPES OF LOGISTIC REGRESSION

# Binary â†’ 0 or 1
# Multinomial â†’ 3+ classes
# One-vs-Rest strategy

# Example:

# Low / Medium / High salary category

# ðŸŸ  8ï¸âƒ£ DOES IT NEED SCALING?

# Yes, usually âœ”

# Logistic Regression performs better when features are scaled.

# ðŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: Why is it called Logistic Regression?
# Because it uses logistic (sigmoid) function.

# Q2: What loss function is used?
# Log Loss / Cross Entropy.

# Q3: Output range?
# Between 0 and 1.

# Q4: Difference between Softmax and Sigmoid?
# Sigmoid â†’ Binary classification
# Softmax â†’ Multi-class classification

# Q5: Can Logistic Regression overfit?
# Yes. Use regularization (L1, L2).

# ðŸŸ¢ 10ï¸âƒ£ REAL PROJECT CONNECTION (Your Job Portal)

# Example:

# Predict:

# Will candidate get selected?

# Features:

# Experience
# Skill Score
# Projects
# CGPA

# Output:

# 0 â†’ Not Selected
# 1 â†’ Selected

# Model gives probability:

# 0.82 â†’ 82% chance of selection

# Thatâ€™s powerful ðŸ”¥

# ðŸ”¥ 11ï¸âƒ£ REGULARIZATION IN LOGISTIC REGRESSION

# It supports:

# L1 (Lasso)
# L2 (Ridge)

# In sklearn:

# LogisticRegression(penalty='l2')

# 12ï¸âƒ£ FINAL SUMMARY

# Linear Regression â†’ Predict number

# Logistic Regression â†’ Predict probability â†’ Convert to class

# Uses Sigmoid
# Uses Log Loss
# Used for classification
