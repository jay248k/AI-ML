import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Pass":  [0,0,0,0,1,1,1,1]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Pass"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = DecisionTreeClassifier(max_depth=3)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Imagine you are selecting a candidate.

# You create rules:

# If Experience > 3
# â€ƒIf Skill Score > 80 â†’ Selected
# â€ƒElse â†’ Not Selected
# Else â†’ Not Selected

# That rule-based system is a Decision Tree.

# So:

# DecisionTreeClassifier = IF-ELSE rule model ğŸŒ³

# ğŸŸ¡ 2ï¸âƒ£ HOW IT WORKS

# Step 1: Choose best feature to split
# Step 2: Split dataset into groups
# Step 3: Repeat splitting
# Step 4: Stop when data becomes pure

# â€œPureâ€ means:

# All samples in node belong to same class.

# ğŸ”µ 3ï¸âƒ£ HOW DOES IT CHOOSE BEST SPLIT?

# For classification, it uses:

# ğŸ‘‰ Gini Index (default)
# ğŸ‘‰ Entropy (Information Gain)

# ğŸ”· Gini Index

# Formula:

# Gini=1-âˆ‘pÂ²

# If node is pure â†’ Gini = 0

# Lower Gini = Better split

# ğŸ”· Entropy

# Formula:

# Gini = 1-âˆ‘pâ‚‚

# If node is pure â†’ Gini = 0

# Lower Gini = Better split

# ğŸ”· Entropy

# Formula:

# Entropy=-âˆ‘plog(p)

# Entropy measures randomness.

# Lower entropy = Better split

# Difference:

# Gini â†’ Faster
# Entropy â†’ Slightly more precise

# In sklearn:

# criterion="gini"
# criterion="entropy"

# 5ï¸âƒ£ IMPORTANT PARAMETERS

# max_depth â†’ Limits tree height

# min_samples_split â†’ Minimum samples to split

# min_samples_leaf â†’ Minimum samples in leaf

# criterion â†’ gini or entropy

# If not controlled â†’ Overfitting

# âš« 6ï¸âƒ£ OVERFITTING PROBLEM

# Decision Trees:

# Very powerful
# Very flexible

# But:

# If tree grows fully â†’ Memorizes training data

# Training accuracy = 100%
# Test accuracy = Low

# Solution:

# âœ” Set max_depth
# âœ” Use pruning
# âœ” Use RandomForest

# ğŸŸ  7ï¸âƒ£ DOES IT NEED SCALING?

# No âŒ

# Trees donâ€™t use distance or gradient.

# Very important interview point ğŸ”¥

# +----------------------+-----------------+
# | Logistic Regression  | Decision Tree   |
# +----------------------+-----------------+
# | Linear boundary      | Non-linear      |
# | Uses sigmoid         | Uses splits     |
# | Needs scaling        | No scaling      |
# | Stable               | Can overfit     |
# +----------------------+-----------------+

# 9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What is Gini Index?
# Measure of impurity.

# Q2: What is entropy?
# Measure of randomness.

# Q3: Why does Decision Tree overfit?
# Because it keeps splitting until pure.

# Q4: Is Decision Tree parametric?
# No.

# Q5: Does it handle categorical data?
# Yes.

# ğŸŸ¢ 10ï¸âƒ£ REAL PROJECT (Your Job Portal)

# Predict:

# Will candidate be selected?

# Tree might create rules:

# If Experience > 2
# â€ƒIf Projects > 3 â†’ Selected
# Else â†’ Not Selected

# Very interpretable.

# HR people like this model because they can see rules.

# ğŸ”¥ 11ï¸âƒ£ SUMMARY

# DecisionTreeClassifier:

# âœ” Rule-based model
# âœ” Uses Gini or Entropy
# âœ” No scaling needed
# âœ” Handles non-linear data
# âŒ Can overfit

# Now you have learned:

# Regression models
# Classification models
# Tree models
# Ensemble model
# Distance-based model