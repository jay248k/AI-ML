import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Pass":  [0,0,0,0,1,1,1,1]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Pass"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Imagine this:

# You move to a new area.

# You want to know:

# Is this area rich or middle class?

# You ask your 5 nearest neighbors.

# If most are rich â†’ You say rich
# If most are middle class â†’ You say middle class

# That is exactly how KNN works.

# KNN = Look at nearest K neighbors â†’ Take majority vote.

# ğŸŸ¡ 2ï¸âƒ£ HOW IT WORKS

# Step 1: Choose value of K (like 3 or 5)

# Step 2: Calculate distance between new point and all training points

# Step 3: Pick K closest points

# Step 4: Majority vote decides class

# For regression â†’ average
# For classification â†’ majority vote

# ğŸ”µ 3ï¸âƒ£ HOW DISTANCE IS CALCULATED?

# Most common:

# ğŸ‘‰ Euclidean Distance

# Formula:

# d = âˆš((x2 - x1)Â² + (y2 - y1)Â²)

# Other distances:

# Manhattan distance
# Minkowski distance

# 5ï¸âƒ£ IMPORTANT PARAMETERS

# n_neighbors â†’ Value of K

# If K small â†’ High variance (overfitting)
# If K large â†’ High bias (underfitting)

# weights:

# uniform â†’ equal vote
# distance â†’ closer neighbors have more importance

# âš« 6ï¸âƒ£ VERY IMPORTANT: SCALING REQUIRED

# Yes âœ”

# Because KNN uses distance.

# If one feature has large values, it dominates.

# Always use:

# StandardScaler or MinMaxScaler before KNN.

# ğŸŸ  7ï¸âƒ£ DECISION BOUNDARY

# KNN can create:

# Very flexible boundaries.

# Unlike Logistic Regression (linear boundary),
# KNN can create curved boundaries.

# ğŸŸ¤ 8ï¸âƒ£ ADVANTAGES

# âœ” Simple to understand
# âœ” No training phase (lazy learner)
# âœ” Works well for small datasets
# âœ” Non-linear

# ğŸ”µ 9ï¸âƒ£ DISADVANTAGES

# âŒ Slow prediction for large data
# âŒ Sensitive to scaling
# âŒ Sensitive to noise
# âŒ Memory expensive

# ğŸ§  10ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: Why is KNN called lazy learner?
# Because it does not train model. It stores data and calculates during prediction.

# Q2: What happens if K = 1?
# Model overfits.

# Q3: What happens if K is very large?
# Model underfits.

# Q4: Does KNN need scaling?
# Yes, very important.

# Q5: How to choose best K?
# Use cross-validation.

# ğŸŸ¢ 11ï¸âƒ£ REAL PROJECT CONNECTION (Your Job Portal)

# Suppose candidate features:

# Experience
# Skill Score
# Projects

# New candidate comes.

# KNN finds 5 similar candidates.

# If most were selected â†’ Predict selected.

# Very intuitive approach.

# ğŸ”¥ 12ï¸âƒ£ SUMMARY

# KNN = Look at nearest neighbors

# Classification â†’ Majority vote
# Regression â†’ Average

# Needs scaling
# Works well for small data
# No actual training