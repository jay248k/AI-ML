import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Pass":  [0,0,0,0,1,1,1,1]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Pass"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = GaussianNB()
model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# ğŸ“˜ Naive Bayes

# This algorithm is:

# âœ” Simple
# âœ” Fast
# âœ” Powerful for text data
# âœ” Very common in interviews

# ğŸŸ¢ 1ï¸âƒ£ VERY EASY EXPLANATION

# Imagine:

# You receive an email.

# You check words like:

# "Free"
# "Win"
# "Offer"

# If many spam words appear â†’ Email is Spam.

# Naive Bayes works exactly like this.

# It calculates probability and chooses the class with highest probability.

# ğŸŸ¡ 2ï¸âƒ£ WHY CALLED "NAIVE"?

# Because it assumes:

# ğŸ‘‰ All features are independent of each other.

# Example:

# In job selection:

# Experience
# Skill Score
# Projects

# Naive Bayes assumes:

# These features do not depend on each other.

# In real life, they may depend.

# But model still works surprisingly well.

# Thatâ€™s why itâ€™s called â€œNaiveâ€.

# ğŸ”µ 3ï¸âƒ£ BAYES THEOREM (Core Formula)

# P(A|B) = P(B|A) * P(A) / P(B)

# In ML terms:

# P(Class|Features) = P(Features|Class) * P(Class) / P(Features)

# Model predicts:

# Class with highest probability.

# ğŸŸ£ 4ï¸âƒ£ TYPES OF NAIVE BAYES

# 1ï¸âƒ£ Gaussian Naive Bayes
# â†’ For continuous data

# 2ï¸âƒ£ Multinomial Naive Bayes
# â†’ For text classification (most common)

# 3ï¸âƒ£ Bernoulli Naive Bayes
# â†’ For binary features (0/1)

# 6ï¸âƒ£ DOES IT NEED SCALING?

# Usually âŒ No

# Naive Bayes is probability-based.

# Scaling is not required in most cases.

# âš« 7ï¸âƒ£ ADVANTAGES

# âœ” Very fast
# âœ” Works well with small datasets
# âœ” Excellent for text classification
# âœ” Low computational cost

# ğŸŸ  8ï¸âƒ£ DISADVANTAGES

# âŒ Assumes independence (not realistic)
# âŒ Not good for highly correlated features
# âŒ Less flexible than tree models

# ğŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: Why is it called Naive?
# Because it assumes features are independent.

# Q2: What theorem is used?
# Bayes Theorem.

# Q3: Which Naive Bayes is best for text?
# Multinomial Naive Bayes.

# Q4: Does Naive Bayes overfit easily?
# No, usually low variance model.

# Q5: Does it need scaling?
# No.

# ğŸŸ¢ 10ï¸âƒ£ REAL PROJECT CONNECTION (Your Job Portal)

# Suppose:

# You want to classify resumes as:

# Shortlisted / Not Shortlisted

# Based on keywords:

# Python
# React
# Machine Learning
# Node.js

# Naive Bayes is excellent for this.

# Very commonly used in:

# âœ” Spam detection
# âœ” Sentiment analysis
# âœ” Resume screening
# âœ” News classification

# 11ï¸âƒ£ NAIVE BAYES vs SVM
# +----------------------+---------------------------------+
# | Naive Bayes          | SVM                             |
# +----------------------+---------------------------------+
# | Probability-based    | Margin-based                    |
# | Very fast            | Slower                          |
# | Works great for text | Works great for structured data |
# | Assumes independence | No such assumption              |
# +----------------------+---------------------------------+

# 12ï¸âƒ£ SUMMARY

# Naive Bayes:

# âœ” Based on probability
# âœ” Uses Bayes Theorem
# âœ” Assumes independence
# âœ” Fast and simple
# âœ” Great for text data