# Linear Regression â†’ uses everything
# Ridge â†’ reduces weight but keeps everything
# Lasso â†’ removes useless features (weight = 0)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Create dataset
data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Marks": [35,40,50,55,60,65,75,80]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Marks"]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create model
model = LinearRegression()

# Train model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

print("Predictions:", y_pred)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Imagine this:

# More hours of study â†’ More marks.

# If we draw a straight line through points, it may look like:

# Marks = 5 Ã— Hours + 30

# That straight-line relationship is called:

# ğŸ‘‰ Linear Regression

# It predicts a number using a straight line.

# Simple meaning:

# It finds the BEST straight line that fits the data.

# 3ï¸âƒ£ MATHEMATICAL FORMULA

# Simple Linear Regression:

# y = mx + b

# Where:

# y = predicted output

# x = input feature

# m = slope (weight)

# c = intercept

# ğŸ”´ 4ï¸âƒ£ HOW MODEL LEARNS (Important)

# Model tries to minimize error.

# Error = Actual âˆ’ Predicted

# Loss Function used:
# Mean Squared Error (MSE) = (1/n) Î£ (yáµ¢ âˆ’ Å·áµ¢)Â²

# This is called:

# ğŸ‘‰ Least Squares Method

# It finds values of m and c that make error smallest.

# ğŸŸ£ 5ï¸âƒ£ MULTIPLE LINEAR REGRESSION

# If more than one feature:

# Experience
# Skills
# Education

# Then formula becomes:
# y = mâ‚xâ‚ + mâ‚‚xâ‚‚ + ... + mâ‚™xâ‚™ + c

# In matrix form:
# Y = XW + C

# Where:
# Where:

# X = feature matrix

# Î² = weights

# ğŸŸ¤ 6ï¸âƒ£ ASSUMPTIONS (Interview Important)

# Linear regression assumes:

# Linear relationship

# No multicollinearity

# Homoscedasticity (constant variance)

# Errors are normally distributed

# If these break 

# â†’ performance drops.

# âš« 7ï¸âƒ£ WHEN TO USE?

# Use when:

# âœ” Output is numeric
# âœ” Relationship is roughly linear

# Examples:

# Salary prediction

# House price prediction

# Marks prediction

# Revenue prediction

# ğŸ”µ WHEN NOT TO USE?

# âŒ If relationship is non-linear
# âŒ Complex patterns
# âŒ Classification problems

# Then use:

# Polynomial Regression

# Decision Trees

# Random Forest

# ğŸŸ¡ 8ï¸âƒ£ IMPORTANT ATTRIBUTES

# After training:

# model.coef_

# Gives slope (m)

# model.intercept_

# Gives intercept (c)

# So final equation becomes:

# Marks = m Ã— Hours + c

# ğŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: Why is it called linear?
# Because output is linear combination of inputs.

# Q2: What is overfitting in linear regression?
# When model fits noise instead of pattern.

# Q3: Difference between Linear and Logistic Regression?
# Linear â†’ continuous output
# Logistic â†’ classification (0 or 1)

# Q4: What if features are correlated?
# It causes multicollinearity problem.

# ğŸŸ¢ 10ï¸âƒ£ REAL-WORLD (Your Resume Project)

# Suppose resume score depends on:

# Experience
# Skill score
# CGPA

# Model learns:

# Score = 2Ã—Experience + 0.5Ã—Skill + 1.2Ã—CGPA + 10

# Then predicts ranking score.

# ğŸŸ£ 11ï¸âƒ£ BIG PICTURE

# Machine Learning Flow:

# Data â†’ Clean â†’ Split â†’ Train â†’ Predict â†’ Evaluate â†’ Improve

# Linear Regression is foundation of ML.

# Understanding this deeply makes everything easier.