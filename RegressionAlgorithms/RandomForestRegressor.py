import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Dataset
data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Marks": [35,40,50,55,60,65,75,80]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Marks"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestRegressor(
    n_estimators=100, 
    max_depth=3,
    random_state=42
)

model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Decision Tree = One smart student

# Random Forest = 100 smart students voting together ğŸ˜

# Instead of building ONE tree, it builds MANY trees.

# Final prediction = Average of all tree predictions.

# So:

# Random Forest = Many Decision Trees + Averaging

# ğŸŸ¡ 2ï¸âƒ£ WHY WE NEED IT?

# Problem with Decision Tree:

# âŒ Overfitting
# âŒ High variance

# Random Forest fixes this by:

# âœ” Using multiple trees
# âœ” Using different random samples
# âœ” Using different random features

# Result:

# More stable
# Better generalization
# Less overfitting

# ğŸ”µ 3ï¸âƒ£ HOW IT WORKS (STEP-BY-STEP)

# Step 1: Take random sample from dataset (Bootstrap sampling)

# Step 2: Build a decision tree on that sample

# Step 3: At each split, choose random subset of features

# Step 4: Repeat this process many times

# Step 5: Final prediction = Average of all trees

# Formula:

# Prediction=1/Nâˆ‘Tree_i(X)

# 5ï¸âƒ£ IMPORTANT PARAMETERS

# n_estimators â†’ Number of trees (default 100)

# max_depth â†’ Controls tree size

# min_samples_split â†’ Minimum samples to split

# min_samples_leaf â†’ Minimum samples per leaf

# max_features â†’ Number of features considered per split

# ğŸ”´ 6ï¸âƒ£ WHY RANDOM FOREST IS POWERFUL

# Because of:

# âœ” Bagging (Bootstrap Aggregation)
# âœ” Random feature selection
# âœ” Averaging reduces variance

# This makes it:

# More accurate than single tree
# Less overfitting
# Robust

# 7ï¸âƒ£ DECISION TREE vs RANDOM FOREST

# +----------------+-----------------+
# | Decision Tree  | Random Forest   |
# +----------------+-----------------+
# | One tree       | Many trees      |
# | High variance  | Low variance    |
# | Overfits easily| More stable     |
# | Fast           | Slower          |
# | Less accurate  | More accurate   |
# +----------------+-----------------+

# 8ï¸âƒ£ DOES IT NEED SCALING?

# No âŒ

# Trees do not require feature scaling.

# Very important interview point ğŸ”¥

# ğŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What is bagging?
# Answer: Training multiple models on random subsets and averaging results.

# Q2: Why does Random Forest reduce overfitting?
# Because averaging multiple trees reduces variance.

# Q3: What happens if n_estimators increases?
# Better performance (usually), but slower.

# Q4: Does Random Forest perform feature selection?
# Yes, indirectly via feature importance.

# ğŸŸ¢ 10ï¸âƒ£ FEATURE IMPORTANCE (Very Important)

# Random Forest can tell:

# Which feature is most important.

# Example:

# model.feature_importances_

# In your Job Portal:

# Experience â†’ 0.40
# Skills â†’ 0.35
# Projects â†’ 0.20
# CGPA â†’ 0.05

# This is powerful for analysis ğŸ”¥

# ğŸŸ£ 11ï¸âƒ£ WHEN TO USE?

# Use Random Forest when:

# âœ” Complex data
# âœ” Non-linear relationships
# âœ” Many features
# âœ” You want high accuracy
# âœ” Decision Tree overfits

# It works very well in:

# Salary prediction

# Resume ranking

# Credit scoring

# House price prediction

# ğŸ”¥ 12ï¸âƒ£ SUMMARY

# Decision Tree = One brain

# Random Forest = Many brains voting

# Less overfitting
# Better performance
# No scaling needed
# Handles non-linearity