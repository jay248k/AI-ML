import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split

# Dataset
data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Marks": [35,40,50,55,60,65,75,80]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Marks"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = DecisionTreeRegressor(max_depth=3)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Linear Regression draws a straight line.

# But what if data is NOT straight?

# Example:

# Hours studied â†’ Marks

# Maybe:

# 0â€“2 hours â†’ 30 marks
# 3â€“5 hours â†’ 50 marks
# 6â€“8 hours â†’ 80 marks

# This is not a straight line.

# Decision Tree says:

# ğŸ‘‰ â€œI will split the data into groups.â€

# It creates rules like:

# If Hours â‰¤ 2 â†’ Predict 30
# If 2 < Hours â‰¤ 5 â†’ Predict 50
# If Hours > 5 â†’ Predict 80

# So:

# Decision Tree = Series of IF-ELSE rules ğŸŒ³

# ğŸŸ¡ 2ï¸âƒ£ HOW IT WORKS (STEP-BY-STEP)

# Step 1: Choose best feature to split
# Step 2: Divide data into two parts
# Step 3: Repeat splitting
# Step 4: Stop when condition met

# Final structure looks like:

#           Hours â‰¤ 5?
#            /      \
#         Yes        No
#       Marks=50   Marks=80

# Each end node = prediction value.

# ğŸ”µ 3ï¸âƒ£ HOW DOES IT CHOOSE BEST SPLIT?

# For regression, it uses:

# ğŸ‘‰ MSE (Mean Squared Error)

# It tries to split data such that:

# Variance inside each group is minimum.

# Formula:

# MSE = (1/n) * âˆ‘(yáµ¢ - Å·)Â²

# Tree chooses split that reduces MSE most.

# 5ï¸âƒ£ IMPORTANT PARAMETERS

# max_depth â†’ Controls tree height
# min_samples_split â†’ Minimum samples to split
# min_samples_leaf â†’ Minimum samples per leaf
# max_features â†’ Features to consider

# If you donâ€™t control these â†’ Overfitting.

# ğŸ”´ 6ï¸âƒ£ BIG PROBLEM: OVERFITTING

# Decision Trees are powerful.

# Too powerful sometimes.

# If tree grows too deep:

# It memorizes training data.

# Training accuracy = 100%
# Test accuracy = Poor

# Solution:

# âœ” Set max_depth
# âœ” Use pruning
# âœ” Use RandomForest

# 7ï¸âƒ£ LINEAR REGRESSION vs DECISION TREE

# +-------------------------------+--------------------------+
# | Linear Regression             | Decision Tree            |
# +-------------------------------+--------------------------+
# | Straight line                 | Tree rules               |
# | Assumes linear                | No assumption            |
# | Simple                        | Flexible                 |
# | Low variance                  | High variance            |
# | Cannot handle complex patterns| Handles complex patterns |
# +-------------------------------+--------------------------+

# 8ï¸âƒ£ WHEN TO USE?

# Use Decision Tree when:

# âœ” Relationship is non-linear
# âœ” Data is complex
# âœ” You want interpretability (rules)
# âœ” Mixed features (numeric + categorical)

# Example:

# Job candidate score prediction.

# If:

# Experience > 3 years AND Skill score > 80
# â†’ High score

# Tree handles this naturally.

# ğŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What metric does DecisionTreeRegressor use?
# Answer: MSE / Variance reduction

# Q2: Why does Decision Tree overfit?
# Because it keeps splitting until pure nodes.

# Q3: How to prevent overfitting?
# Limit depth, pruning, RandomForest.

# Q4: Is scaling required?
# No. Trees donâ€™t need scaling.

# Important point ğŸ”¥

# ğŸŸ¢ 10ï¸âƒ£ REAL PROJECT CONNECTION (Your Job Portal)

# Suppose you predict resume score.

# Tree might create rules:

# If Experience > 5
# If Projects > 3
# Score = High
# Else
# Score = Medium

# Very practical.

# ğŸ”¥ 11ï¸âƒ£ SUMMARY

# Linear Regression â†’ Draws line

# Decision Tree â†’ Creates rule-based structure

# No scaling needed
# Handles non-linear patterns
# Can overfit easily