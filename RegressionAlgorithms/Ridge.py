# Linear Regression â†’ uses everything
# Ridge â†’ reduces weight but keeps everything
# Lasso â†’ removes useless features (weight = 0)


import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split

# Sample dataset
data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Marks": [35,40,50,55,60,65,75,80]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Marks"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = Ridge(alpha=1.0)  # alpha = lambda
model.fit(X_train, y_train)

predictions = model.predict(X_test)

print("Predictions:", predictions)
print("Weights:", model.coef_)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Linear Regression tries to fit:

# y = w1*x1 + w2*x2 + ... + wn*xn + b

# But sometimes:

# ğŸ‘‰ Model gives VERY BIG weight values
# ğŸ‘‰ Model overfits
# ğŸ‘‰ Features are highly correlated (multicollinearity)

# So we control weights.

# Ridge Regression = Linear Regression + Penalty for large weights.

# It says:

# â€œOkayâ€¦ you can learn weightsâ€¦ but donâ€™t make them too big.â€

# ğŸŸ¡ 2ï¸âƒ£ WHY WE NEED RIDGE?

# Problem 1: Overfitting
# Problem 2: Multicollinearity
# Problem 3: Large variance

# Example:

# Experience and Age are highly correlated.

# Linear regression becomes unstable.

# Ridge stabilizes it.

# ğŸ”µ 3ï¸âƒ£ MATHEMATICAL FORMULA (Important)

# Normal Linear Regression Loss:
# Loss = âˆ‘(yáµ¢ - Å·)Â²

# Ridge Regression adds penalty:
# Loss = âˆ‘(yáµ¢ - Å·)Â² + Î±âˆ‘wâ±¼Â²

# Where:

# Where:

# Î» (lambda) = regularization parameter

# wÂ² = square of weights

# This is called:

# ğŸ‘‰ L2 Regularization

# ğŸŸ£ 4ï¸âƒ£ WHAT Î» (Lambda) DOES

# If:

# Î» = 0 â†’ Same as Linear Regression

# Î» small â†’ Small penalty

# Î» big â†’ Strong penalty â†’ weights shrink

# Important:

# Ridge makes weights SMALL
# BUT never exactly ZERO

# In sklearn:

# alpha = Î»

# ğŸ”´ 6ï¸âƒ£ DIFFERENCE: LINEAR vs RIDGE

# +--------------------+----------------------+
# | Linear Regression  | Ridge                |
# +--------------------+----------------------+
# | No penalty         | Has penalty          |
# | Can overfit        | Reduces overfitting  |
# | Weights large      | Weights shrink       |
# | No regularization  | L2 regularization    |
# +--------------------+----------------------+

# 7ï¸âƒ£ GEOMETRIC INTUITION (Advanced)

# Linear Regression:

# Minimizes error only.

# Ridge:

# Minimizes error inside a circle constraint.

# That constraint forces weights to stay small.

# ğŸŸ  8ï¸âƒ£ WHEN TO USE RIDGE?

# Use Ridge when:

# âœ” Many features
# âœ” Multicollinearity exists
# âœ” Linear regression overfits
# âœ” All features are important

# ğŸŸ¡ 9ï¸âƒ£ WHEN NOT TO USE RIDGE?

# If you want:

# Feature selection

# Because Ridge never makes weight zero.

# For feature selection use:

# ğŸ‘‰ Lasso Regression

# ğŸ§  10ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What type of regularization does Ridge use?
# Answer: L2 Regularization

# Q2: What happens if alpha increases?
# Weights shrink more.

# Q3: Does Ridge perform feature selection?
# No.

# Q4: What problem does Ridge solve?
# Multicollinearity and overfitting.

# ğŸŸ¢ 11ï¸âƒ£ REAL WORLD EXAMPLE (Resume Ranking)

# Suppose you build ML model in your Job Portal:

# Features:

# Experience
# Skill Score
# Projects
# Certifications

# Linear regression might give:

# Experience = 100
# Skill Score = 0.0001

# Unstable.

# Ridge balances them properly.

# ğŸ”¥ 12ï¸âƒ£ SIMPLE SUMMARY

# Linear Regression = Fit best line

# Ridge = Fit best line + Donâ€™t allow crazy weights

# It makes model more stable.