# Linear Regression â†’ uses everything
# Ridge â†’ reduces weight but keeps everything
# Lasso â†’ removes useless features (weight = 0)

import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split

data = {
    "Hours": [1,2,3,4,5,6,7,8],
    "Marks": [35,40,50,55,60,65,75,80]
}

df = pd.DataFrame(data)

X = df[["Hours"]]
y = df["Marks"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = Lasso(alpha=0.5)
model.fit(X_train, y_train)

print("Weight:", model.coef_)
print("Intercept:", model.intercept_)

# 1ï¸âƒ£ VERY EASY EXPLANATION

# Linear Regression â†’ no control on weights

# Ridge â†’ makes weights small

# Lasso â†’ makes some weights ZERO

# That means:

# ğŸ‘‰ Lasso automatically removes useless features.

# So:

# Lasso = Linear Regression + Feature Selection

# ğŸŸ¡ 2ï¸âƒ£ WHY WE NEED LASSO?

# Imagine your dataset has:

# Experience

# Skill score

# CGPA

# Random noise column

# Useless feature

# Linear Regression â†’ uses everything
# Ridge â†’ reduces weight but keeps everything
# Lasso â†’ removes useless features (weight = 0)

# That is powerful ğŸ”¥

# ğŸ”µ 3ï¸âƒ£ MATHEMATICAL FORMULA

# Normal Linear Regression:

# Loss=âˆ‘(yáµ¢ - Å·)Â²

# Lasso adds penalty:
# Loss=âˆ‘(yáµ¢ - Å·)Â² + Î±âˆ‘|wâ±¼|

# Notice:

# Ridge â†’ wÂ²
# Lasso â†’ |w|

# This is called:

# ğŸ‘‰ L1 Regularization

# ğŸ”´ 4ï¸âƒ£ WHAT Î» (alpha) DOES

# If:

# Î» = 0 â†’ Same as Linear Regression

# Î» small â†’ Small penalty

# Î» big â†’ More weights become 0

# In sklearn:

# alpha = Î»

# If you had multiple features, some coefficients might become 0.

# ğŸŸ¤ 6ï¸âƒ£ LASSO vs RIDGE (Very Important)

# +---------------------------------+----------------------------------+
# | Ridge                           | Lasso                            |
# +---------------------------------+----------------------------------+
# | L2 penalty                      | L1 penalty                       |
# | Shrinks weights                 | Makes some weights zero          |
# | No feature selection            | Yes feature selection            |
# | Good when all features important| Good when many useless features  |
# +---------------------------------+----------------------------------+

# 7ï¸âƒ£ GEOMETRIC INTUITION (Advanced Understanding)

# Ridge constraint = Circle
# Lasso constraint = Diamond

# Diamond shape touches axis â†’ some weights become exactly zero.

# Thatâ€™s why Lasso removes features.

# âš« 8ï¸âƒ£ WHEN TO USE LASSO?

# Use Lasso when:

# âœ” Many features
# âœ” Some features useless
# âœ” Need automatic feature selection
# âœ” High-dimensional data

# Example:

# Resume screening model with 100+ features.

# Lasso selects important ones.

# ğŸ§  9ï¸âƒ£ INTERVIEW QUESTIONS

# Q1: What type of regularization does Lasso use?
# Answer: L1 regularization

# Q2: Why does Lasso perform feature selection?
# Because L1 penalty can shrink weights to exactly zero.

# Q3: Which is better Ridge or Lasso?
# Depends:

# All features important â†’ Ridge

# Many useless features â†’ Lasso

# Q4: What is ElasticNet?
# Combination of Ridge + Lasso

# ğŸ”¥ 10ï¸âƒ£ REAL-WORLD (Your Job Portal Idea)

# Suppose features:

# Experience
# Projects
# Certifications
# CGPA
# Github stars
# Random column

# Lasso might output:

# Experience = 2.3
# Projects = 1.5
# Certifications = 0
# CGPA = 0
# Github stars = 1.2

# See?

# It removed useless features automatically.

# ğŸŸ¢ 11ï¸âƒ£ FINAL SUMMARY

# Linear Regression â†’ No control

# Ridge â†’ Control weight size

# Lasso â†’ Control weight size + Remove useless features